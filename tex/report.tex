\documentclass[]{article}
\usepackage{times}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{graphicx}
\usepackage{float}
\usepackage{listings}
\usepackage{pgf}
\usepackage{tikz}
\usepackage{subfig}
\usepackage{pgfplots}
\usepackage{filecontents}
\usetikzlibrary{arrows,automata}
\usepackage[latin1]{inputenc}
\usepackage[round]{natbib}
\lstset{
  basicstyle=\fontfamily{lmvtt}\selectfont\small,
  columns=fullflexible,
}

%opening
\title{CS3105: Artificial Intelligence - The Imitation Game}
\author{ID: 150013828}

\begin{document}

\maketitle

%\begin{figure}[!htbp]
%	\centering
%	\begin{minipage}[b]{0.4\textwidth}
%		\begin{tikzpicture}
%		\begin{axis}[ xmin=0, xmax=15, xlabel=Nodes, ylabel=Time, legend style={at={(0.5,-0.1)},anchor=north} ]
%		\addplot table [x=nodes, y=dst, col sep=comma] {movie.csv};
%		\addlegendentry{dst}
%		\addplot table [x=nodes, y=scp, col sep=comma] {movie.csv};
%		\addlegendentry{scp}
%		\addplot table [x=nodes, y=sftp, col sep=comma] {movie.csv};
%		\addlegendentry{sftp}
%		\end{axis}
%		\end{tikzpicture}
%		\caption{movie.mjpeg - 4.1M\label{fig:compar3}}
%	\end{minipage}
%	\hfill
%	\begin{minipage}[b]{0.4\textwidth}
%		\begin{tikzpicture}
%		\begin{axis}[ xmin=0, xmax=15, xlabel=Nodes, ylabel=Time, legend style={at={(0.5,-0.1)},anchor=north} ]
%		\addplot table [x=nodes, y=dst, col sep=comma] {tfatf.csv};
%		\addlegendentry{dst}
%		\addplot table [x=nodes, y=scp, col sep=comma] {tfatf.csv};
%		\addlegendentry{scp}
%		\addplot table [x=nodes, y=sftp, col sep=comma] {tfatf.csv};
%		\addlegendentry{sftp}
%		\end{axis}
%		\end{tikzpicture}
%		\caption{The Fast and the Furious - 302M\label{fig:compar4}}
%	\end{minipage}
%\end{figure}

\section{Overview} In this report, a chatbot called "bob" is presented. Bob is related very closely to \citeauthor{Weizenbaum}'s ELIZA [\citenum{Weizenbaum}], in that it performas a series of pattern-matching and string manipulations. Bob also has the ability to train a markov chain in an attempt to add extra conversational interest.

There is also a discussion (section \ref{sec:discussion}) about the Turing test and how I find that it fails to capture an ethical measurement of the machine in question - which might be important to intelligence.

\section{Chatbot}
There are two main components (differing techniques) as to how Bob generates text.
\paragraph{String Replacement} In the file "bob.py" one will see all the string replacement rules that define how bob will transform the input string. First a pattern is matched among the "builtinKeys" which looks at overarching sentence structure. The keys map to a formatting string, with formatting variables analagous to regex capture groups for which a "$\lbrace0\rbrace$" for instance will be replaced in the end with the entire matched key; a "$\lbrace1\rbrace$" with the first capture group defined in the key etc.. This process is very similar to that discussed in the initial implementation of ELIZA - \cite{Weizenbaum}[\citenum{Weizenbaum}]. It also follows in ELIZA's footsteps by attempting to imitate some sort of Rogerian psychotherapist (although as we'll see in \ref{sec:conv1} - it might be Bob who needs the psychotherapy).

\paragraph{Markov Chains} In "markovbot.py" a bot is described which does something very different to the aforementioned technique. The bot must first be \emph{trained} on a plain text file. In this training process, coincident n-grams are mapped to each other - in this implementation, a bigram on the left maps to a single word on the right.
\\\\
This process was brought to my attention by a very helpful \emph{Stack Overflow} post, which gives a conceptual overview of this process, answered by the user "Nocker": (http://stackoverflow.com/questions/5306729/how-do-markov-chain-chatbots-work).
\\\\
This then successively moves through the file. E.g. "The quick brown fox jumps..." is broken into the following relations:
\begin{itemize}
	\item "The quick" $\rightarrow$ "brown"
	\item "quick brown" $\rightarrow$ "fox"
	\item "brown fox" $\rightarrow$ "jumps"
\end{itemize}
... and so on.
\\\\
The result of this training process is a map which can be called upon, given any bigram as a key. When generating the text, first a random bigram is chosen, and then as these tokens build up a buffer, the next key defines itself.

\paragraph{NOTE} I do not filter out the training file for foul language, and I'm almost certain the mentioned data set [\citenum{Danescu-Niculescu-Mizil+Lee:11a}] contains some - if this is something you care about. Just to note that, the program itself will not create swears voluntarily, but it recall some upon using the map generated by the training file.

\section{Examples}

For these examples, the markov chain is trained using n-grams of the text utterances from a large corpus provided by \cite{Danescu-Niculescu-Mizil+Lee:11a}. The corpus comprises of movie dialogue which should provide the markov chain a reasonable vocabulary for conversations, which is what we want. A python script \emph{importcorpus.py} is provided which preprocesses the raw provided text into something more suitable for n-gram extraction (a fair proportion of the provided text is specifically for taxonomy). One could use the script to recreate the testing conditions described here.
\\\\
There is also a setup script which automatically downloads the corpus and performs the aforementioned preprocessing for use in bob:
\begin{lstlisting}[language=bash, frame=single]
bash setup.sh
\end{lstlisting}

\subsection{Running Bob}

A makefile is also present which makes the process of setup \emph{even easier}. Simply run:

\begin{lstlisting}[language=bash, frame=single]
make
\end{lstlisting}

...to download the corpus and process it, and then:

\begin{lstlisting}[language=bash, frame=single]
python2 main.py data/corpus.txt
\end{lstlisting}

...will begin training and start the chat session.

\subsection{A Conversation With Bob}\label{sec:conv1}

As it turns out, with the correct training corpus (this is provided in the setup script), Bob takes on a very strange personality.

\begin{verbatim}
you: hello

bob: how are you? The invisible girl ...

you: well, I'm not a girl

bob: please explain further Whatever the reason I here to see you , Sweetheart .

you: ...

bob: what about ...? Denny over Christmas and I had to pawn that to get a 
strangle-hold on it .

you: Who's Denny?

bob: Do you know? Cathedral , all right .

you: no

bob: where do you think this negativity is coming from? A gotta get me 
something I can take care of myself .

you: I don't know

bob: take your time... Interference scan .

you: you're crazy

bob: how do you feel about I'm crazy? An unfortunate phrase , it isn I hungry 
and scared .

you: are you ok?

bob: Do you know? Dad died .

you: that's awful, I'm sorry

bob: That's fine Thousands killed .

you: what!?

bob: What! - I'm not too sure what the answer is... I detect a rebuke ?

you: I'm not sure either...

bob: That's a shame Assassinated two men do it , what all the shootin for , 
will you tell me ?

you: ^C

[you leave abruptly...]

bob: Was it something I said?
\end{verbatim}

\subsection{Second Conversation With Bob}
\begin{verbatim}
you: hi

bob: how are you? Hundred dollars ?

you: yes please

bob: that's interesting Gynde for 10 : a mad dog .

you: Do I not get $100?

bob: why not? Guys better I feel about him .

you: how does this work then?

bob: what's on your mind? Corey ?

you: no, $100

bob: why not? Here another wire , sir .

you: how rude...

bob: tell me about how rude... And bitters and blue ruin you spill out the 
light ?

you: are you going to give me money or not?

bob: why not? Home Mortgage .

you: my mortgage too!?

bob: Do you know? Gallons an hour .

you: err?

bob: Can you tell me? err? Unharmed if necessary .

you: not really

bob: where do you think this negativity is coming from? Didn return from that 
village ?

you: Village? St Andrews you mean?

bob: I think you maybe know more about this than you lead on Slaw it OUT 
RAY-GEOUS !

you: believe me, I think you're clearly the most knowledgeable person here

bob: what makes you feel that way - at eight ...

you: your use of language is so amusing

bob: I see... Giving us a way out .

you: no, no, not at all...

bob: where do you think this negativity is coming from? Inches !

you: I don't think I want to talk to you any more

bob: what would it mean if you had to talk to me any more? Rope this banker 
tomorrow and tell the truth !

you: goodbye

bob: oh? Next Friday .

you: maybe...

bob: how does that make you feel? Racks over there ?

you: ^C

[you leave abruptly...]

bob: Was it something I said?
\end{verbatim}
\subsection{Bob \& CleverBot}

\begin{verbatim}
you: Nice to see you here.

bob: I see... That seriously .

you: Yes :D

bob: what else? Out A soft side .

you: For me to be happy.

bob: what else? Gets here in five minutes ...

you: But how will I know it's you

bob: where do you think this negativity is coming from? The Underworld .

you: I don't know.

bob: take your time... Doing .

you: Come to me.

bob: explain... We !

you: You don't have to know. I just feel empty

bob: where do you think this negativity is coming from? Service history .

you: Outer space.

bob: what else? Play at present .

you: What else what?

bob: I think you maybe know more about this than you lead on Is Gretchen .

you: What do you know about?

bob: where do you think this negativity is coming from? How will I be going .

you: I don't know I just want You to answer that queston.

bob: is me to answer that queston. essential? Often , fell in love ?

you: If you want me to answer your question you must answer to my question 
first.

bob: what about If Me Want You To Answer Your Question Me Must Answer To Your 
Question First.? Fresh new outlook , a hostess and a spoiled brat of a rich 
man .

you: What question?

bob: I think you maybe know more about this than you lead on Underpowered .

you: I know a lot about you.

bob: where do you think this negativity is coming from? Mists of time .

you: 
I think they come from habital worlds that can house water and have a similar 
atmosphere to Earth.
bob: is there anything else I can help you with? True - and the Santa Fe ?

you: 

bob: what's on your mind? Those that would marry King Westley .

you: I don't know. A large mass of things currently.

bob: take your time... You refuse , Oleg !

you: Do you like me?

bob: Do you know? Already went back home .

you: Yes

bob: Yes . Laurie Strode -- Jamie mother -- Taking the train tonight -- and he 
-- remarried his WIFE ?

you: quit

bob: See you later...
\end{verbatim}

As it turns out, CleverBot and Bob are quite hostile around each other when it comes to answering questions.

\section{Evaluation}
Bob performs well in that he's quite funny (due to the given training data), but I would say that in all of these examples, it's clear that Bob's grasp of both correct syntax and semantics are somewhat skewed. The string-replacement section of Bob is not quite deterministic, but given a string of a certain pattern you definitely know with a probability what you are going to get back. However, the markov chain kicks in afterwards and cuts through this predictibility with something that looks structurally sound (because the markov chain works off of coincident n-grams), but can semantically mean nothing. This is because, the words transition in the chain is chosen completely at random - as long as the words are structurally linked then they have some probability of showing up next to each other. On the whole though, it worked surprisingly well. I had tried with other, much shorter corpuses (such as the complete works of shakespeare), but it didn't quite work as well. The film corpus seems perfect for this purpose though.
\\\\
The chat with cleverbot did show more weaknesses in the simple string replacement method I use has no concept of the conversation's context.
\\\\
In a future iteration, I would like to experiment with feeding words from the conversation itself into the markov chain - thus effectively creating new vocabulary and sentence structure on-the-fly.

As well as this, I would like to alleviate the lack of conversational context somehow - not entirely sure what that would consist of. I imagine that, it might involve adding knowledge placement and collection rules alongside the pattern matching / string substitution system.

\section{The Turing Test}\label{sec:discussion}

This section serves as a mini-essay about how I feel chatbots relate to the Turing test, and how the Turing test relates to general AI.

\paragraph{Ethical Concerns \& "The Theological Objection"} The theological objection was given by Turing: "Thinking is a function of man's immortal soul. God has given an immortal soul to every man and woman, but not to any other animal or to machines. Hence no animal or machine can think." [\citenum{Turing}]

While I am in full agreement with his following refutation, I find that perhaps there is a kernel of truth behind this objection - which may sprout into a direction Turing was not anticipating - which is to say that the provided criteria of the \emph{Turing test} do not necessitate what traditionally constitutes a soul, and that "soul-like" criteria might be required in order to fully create something that is \emph{sufficiently intelligent}; even if this is in imitation.

Turing's refutation to this objection was, in summary, to say: \emph{what's stopping a deity from granting machines a soul too?} Like I noted, I am in agreement with that; regardless of whether or not one believes in god, this seems sound. What I would like to argue however is that the Turing test fails to categorize the very soul potentially granted to an intelligent machine. And, that litmus tests, such as the Turing test, are less meaningful if the "soul-like" aspects of intelligence are left out.

In claiming that a machine has no soul (as the theological objection does), without needing not to go over the physical nature of the soul, what is this saying? I think that it is to say:
\begin{enumerate}
	\item a machine carries no "free will"
	\item a machine has a vanishing conception of moral values, let alone that of a "nature" resulting in the collection of those values
	\item a machine carries no "immortal" content beyond the inherent notional facts of its existence
\end{enumerate}

\noindent
Does my poor chatbot, Bob, exhibit any of these? Sadly not. But, has any machine that has ever even come close to passing the Turing test exhibited any of these either? (I would forgive a machine for failing on point 3 however, as it is somewhat more difficult, or less possible to prove).
I think that, of these, \emph{point two} is the most relevant to this discussion. Humans \emph{certainly} have moral values. And, these influence our thoughts and actions. The deepest of one's moral values are very difficult to act against. If future machines, intelligent machines, do not have a similar moral system then what conceivable acts could be undertaken in the name of, or by virtue of, their intelligence - I wonder? Could it be worse than what we are already capable of inflicting on each other for ideological reasons?

It is becoming increasingly obvious in the field of AI that cutting-edge machines, capable of performing more or less autonomously in their environment, \emph{need} to find a focus for the satisfactory resolve of ethical questions. Machines \emph{now} have the capability to leave lives permanently altered as a result of their own decisions: they bear some responsibility for any death concerning driverless-cars (not to discount their inherent safety over a human's ability, but to note that, accidents happen, and that the driverless-cars will occasionally have to make tough decisions enevitably); they have some convinced that they are worthy as social entities\footnote{https://www.theguardian.com/world/2016/nov/20/japan-stranded-singles-virtual-love}\footnote{http://time.com/3998563/virtual-love-japan/}. But, there is a vacuum left behind in the wake of our own magnetic attraction to anthropomorphize the \emph{empty shells of intelligence} that are created today.

If, in the future, a self-driving car was for whatever reason deemed worthy of being "intelligent" then I believe it would have to do a lot more than fool us in conversation, it would have to do something about moral dilemmas such as the \emph{Trolley Problem}.
\\\\

I think we should be weary of the rapid impact AI \emph{could} have on the meaning we all place in our lives. Not to suggest it is 'around the corner', but to suggest that something approximating general, \emph{strong AI} would be inevitable, short of being stopped in it's tracks by a natural disaster or perhaps via some legislative movement. If the AI could always perform a task better than us - or learn how to do it better - would we want to do anything useful - something that is \emph{hard work}? Maybe it is the perfect way to live, maybe it's not - caution, is what I offer here.

There is a feeling that the Turing test is somewhat already defeated, \emph{of course machines could think}, we say, \emph{it's just a matter of time and advancement in processing power} - perhaps even out of hubris to sudden, recent progress in areas such as deep learning neural networks - I think the more powerful question is not "can machines think?", but \emph{can thinking machines live among us} - and to what end?

\bibliography{ref}
\bibliographystyle{plain}

\end{document}